# -*- coding: utf-8 -*-
"""GRIT Doc Cluster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LGhbJY2GtruGoQaM-nN_52Y-7WV2cc0l
"""



# Import necessary libraries
import os
import numpy as np
import pandas as pd
import streamlit as st
from sklearn.cluster import KMeans, DBSCAN
import hdbscan
from ollama import generate
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from PyPDF2 import PdfFileReader
from docx import Document as DocxDocument
import re
import json
import tempfile

# Function to clean text
def clean_text(text):
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    return text

# Function to process uploaded files
def process_uploaded_file(uploaded_file):
    try:
        file_extension = os.path.splitext(uploaded_file.name)[1].lower()
        content = ""  # Initialize content as an empty string
        if file_extension == '.pdf':
            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                tmp_file.write(uploaded_file.read())
                pdf_path = tmp_file.name
                with open(pdf_path, 'rb') as f:
                    reader = PdfFileReader(f)
                    for page_num in range(reader.numPages):
                        page = reader.getPage(page_num)
                        content += page.extract_text()
        elif file_extension == '.txt':
            content = uploaded_file.read().decode("utf-8")
        elif file_extension == '.docx':
            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_file.seek(0)
                doc = DocxDocument(tmp_file.name)
                for paragraph in doc.paragraphs:
                    content += paragraph.text + '\n'
        elif file_extension == '.json':
            content = json.loads(uploaded_file.read().decode("utf-8"))
            content = json.dumps(content)
        else:
            st.warning("Unsupported file type. Please upload a PDF, TXT, DOCX, or JSON file.")
            return None
        return {"file_name": uploaded_file.name, "content": content}
    except Exception as e:
        st.warning(f"Error processing uploaded file '{uploaded_file.name}': {e}")
        return None

# Function to generate embeddings
def generate_embeddings(text, embedding_model_name):
    try:
        # Generate embeddings for the text using ollama
        embedding_data = ollama.embeddings(model=embedding_model_name, prompt=text)
        if embedding_data is not None:
            return embedding_data['embedding']  # Return the embedding
        else:
            return None
    except Exception as e:
        print(f"Error generating embeddings: {e}")
        return None

# Function to suggest clustering parameters using LLM
def suggest_clustering_params(embeddings):
    prompt = f"Based on the distribution of these embeddings: {embeddings}, suggest optimal clustering parameters for K-means, DBSCAN, and HDBSCAN."
    result = generate(model="llama3", prompt=prompt)
    return result['response']

# Function to suggest relevant features using LLM
def suggest_relevant_features(embeddings, feature_names):
    prompt = f"Given these embeddings: {embeddings} and these feature names: {feature_names}, suggest the most relevant features for clustering."
    result = generate(model="llama3", prompt=prompt)
    return result['response']

# Function to perform clustering and visualize results
def perform_clustering_and_visualization(df, file_representations, clustering_params, selected_option):
    if selected_option == "K-means":
        model = KMeans(n_clusters=clustering_params["n_clusters"])
        labels = model.fit_predict(df["file_representation"].tolist())
    else:
        model = hdbscan.HDBSCAN(**clustering_params) if selected_option == "HDBSCAN" else DBSCAN(**clustering_params)
        labels = model.fit_predict(df["file_representation"].tolist())

    df["cluster"] = labels

    # Reduce the file representations to 2 dimensions using PCA
    pca = PCA(n_components=2)
    file_representations_2d = pca.fit_transform(list(file_representations.values()))

    # Plot the data points
    plt.figure(figsize=(10, 6))  # Adjust figure size if needed

    # Iterate over unique clusters and plot points with different colors
    for cluster_label in df['cluster'].unique():
        cluster_mask = df['cluster'] == cluster_label
        plt.scatter(file_representations_2d[cluster_mask, 0], file_representations_2d[cluster_mask, 1],
                    label=f'Cluster {cluster_label}', alpha=0.5)

    plt.title('File Representations Visualization (2D)')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    plt.grid(True)
    plt.legend()
    plt.show()

# Main function to handle the entire process
def main():
    st.title("Document Clustering and Topic Generation")
    st.write("Upload your documents for clustering and topic generation.")

    uploaded_files = st.file_uploader("Upload documents", accept_multiple_files=True)

    if uploaded_files:
        st.write(f"Number of uploaded files: {len(uploaded_files)}")
        with st.spinner(text="Extracting text from the document..."):
            processed_files = []
            failed_files_count = 0
            failed_files_names = []

            for uploaded_file in uploaded_files:
                processed_file = process_uploaded_file(uploaded_file)
                if processed_file:
                    processed_file['content'] = clean_text(processed_file['content'])
                    processed_files.append(processed_file)
                    st.write(f"- {processed_file['file_name']}\n")
                else:
                    failed_files_count += 1
                    failed_files_names.append(uploaded_file.name)

            st.warning(f"Number of failed files: {failed_files_count}")

        CHUNK_SIZE = 512
        chunked_texts = []

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=0,
        )

        file_names = []

        for file_data in processed_files:
            text = file_data['content']
            file_name = file_data['file_name']
            file_names.append(file_name)

            chunks = text_splitter.split_text(text)
            current_chunk_number = 0
            for chunk in chunks:
                current_chunk_number += 1
                chunked_texts.append((file_name, chunk, current_chunk_number))

        if st.button('Start Clustering'):
            st.success("Generating Embeddings")
            progress_bar = st.progress(0)
            progress_bar.text("Generating embeddings...")

            file_embeddings = {}
            for i, chunk_data in enumerate(chunked_texts, start=1):
                file_name, chunk, chunk_number = chunk_data

                embedding = generate_embeddings(chunk, "selected_embedding_model")  # Specify the model name
                embedding = np.array(embedding)

                if file_name not in file_embeddings:
                    file_embeddings[file_name] = []

                file_embeddings[file_name].append(embedding)

                progress_percent = (i / len(chunked_texts))
                progress_bar.progress(progress_percent)

            file_representations = {}
            for file_name, embeddings_list in file_embeddings.items():
                embeddings_array = np.array(embeddings_list)
                average_embedding = np.mean(embeddings_array, axis=0)
                file_representations[file_name] = average_embedding

            df = pd.DataFrame({
                "file_name": file_names,
                "content": [file_data['content'] for file_data in processed_files],
                "file_representation": [file_representations[file_name] for file_name in file_names]
            })

            # Get clustering parameter suggestions from LLM
            embeddings = np.array(list(file_representations.values()))
            params_suggestions = suggest_clustering_params(embeddings)
            st.write("Suggested Clustering Parameters:", params_suggestions)

            # Parse the suggestions (assuming JSON format for simplicity)
            clustering_params = json.loads(params_suggestions)
            selected_option = st.sidebar.selectbox(
                "Select Clustering Algorithm",
                ["K-means", "HDBSCAN", "DBSCAN"],
                index=0  # Default value set to K-means
            )

            perform_clustering_and_visualization(df, file_representations, clustering_params, selected_option)

if __name__ == "__main__":
    main()