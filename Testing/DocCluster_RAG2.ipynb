{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRwDpoTv2Dpn"
      },
      "outputs": [],
      "source": [
        "!pip install -qU datasets ollama openai \"semantic-router[local]\" qdrant-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from semantic_router.encoders import HuggingFaceEncoder\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from tqdm.auto import tqdm\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "data = load_dataset(\"jamescalam/ai-arxiv2-semantic-chunks\", split=\"train[:10000]\")\n",
        "data = data.map(lambda x: {\"id\": x[\"id\"], \"metadata\": {\"title\": x[\"title\"], \"content\": x[\"content\"]}})\n",
        "data = data.remove_columns([\"title\", \"content\", \"prechunk_id\", \"postchunk_id\", \"arxiv_id\", \"references\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding Model\n",
        "encoder = HuggingFaceEncoder(name=\"dwzhu/e5-base-4k\")\n",
        "dims = len(encoder([\"this is a test\"])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cbcdf6c99d943649ddcb4df2ae789d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create embeddings\n",
        "def create_embeddings(data, batch_size=128):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(data), batch_size)):\n",
        "        batch = data[i:i+batch_size]\n",
        "        chunks = [f'{x[\"title\"]}: {x[\"content\"]}' for x in batch[\"metadata\"]]\n",
        "        embeds = encoder(chunks)\n",
        "        embeddings.extend(embeds)\n",
        "    return embeddings\n",
        "\n",
        "embeddings = create_embeddings(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qdrant Setup\n",
        "index_name = \"qdrant-llama-3-rag-1\"\n",
        "client = QdrantClient(\"http://localhost:6333\")\n",
        "if index_name not in [collection.name for collection in client.get_collections().collections]:\n",
        "    client.create_collection(collection_name=index_name, vectors_config=models.VectorParams(size=dims, distance=models.Distance.COSINE))\n",
        "    time.sleep(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a72b5f30c99445789bb7c7f0d573519",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Populate Qdrant with embeddings\n",
        "def populate_qdrant(data, embeddings, batch_size=128):\n",
        "    for i in tqdm(range(0, len(data), batch_size)):\n",
        "        batch = data[i:i+batch_size]\n",
        "        points = [\n",
        "            models.PointStruct(\n",
        "                id=int(id.split('#')[0].replace('.', '')),  # Convert to integer\n",
        "                vector=embed,\n",
        "                payload=metadata\n",
        "            )\n",
        "            for id, embed, metadata in zip(batch[\"id\"], embeddings[i:i+batch_size], batch[\"metadata\"])\n",
        "        ]\n",
        "        client.upsert(collection_name=index_name, points=points)\n",
        "\n",
        "populate_qdrant(data, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from openai import OpenAI\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm_client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
        "MODEL = 'llama3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def suggest_clustering_params():\n",
        "    query = \"Based on the dataset characteristics, suggest optimal hyperparameters for hierarchical clustering, including the number of clusters and linkage type.\"\n",
        "    messages = [{\"role\": \"user\", \"content\": query}]\n",
        "    response = llm_client.chat.completions.create(model=MODEL, messages=messages)\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def parse_params(suggestion):\n",
        "    # Implement parsing logic based on LLM suggestion format\n",
        "    params = {\"n_clusters\": 5, \"linkage\": \"ward\"}\n",
        "    return params\n",
        "\n",
        "suggested_params = suggest_clustering_params()\n",
        "params = parse_params(suggested_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perform_clustering(embeddings, n_clusters, linkage):\n",
        "    model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
        "    labels = model.fit_predict(embeddings)\n",
        "    score = silhouette_score(embeddings, labels)\n",
        "    return labels, score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Silhouette Score: 0.13468380270731145\n"
          ]
        }
      ],
      "source": [
        "\n",
        "labels, score = perform_clustering(embeddings, params['n_clusters'], params['linkage'])\n",
        "print(f\"Silhouette Score: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Iterate based on feedback\n",
        "for _ in range(3):\n",
        "    suggested_params = suggest_clustering_params()\n",
        "    params = parse_params(suggested_params)\n",
        "    labels, score = perform_clustering(embeddings, params['n_clusters'], params['linkage'])\n",
        "    print(f\"Silhouette Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_docs(query: str, top_k: int) -> list[str]:\n",
        "    xq = encoder([query])\n",
        "    search_result = client.search(collection_name=index_name, limit=top_k, query_vector=xq[0], with_payload=True)\n",
        "    return [point.payload['content'] for point in search_result]\n",
        "\n",
        "def generate(query: str, docs: list[str]):\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant that answers questions about AI using the \"\n",
        "        \"context provided below.\\n\\n\"\n",
        "        \"CONTEXT:\\n\"\n",
        "        \"\\n---\\n\".join(docs)\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "    chat_response = llm_client.chat.completions.create(model=MODEL, messages=messages)\n",
        "    return chat_response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkl2S0Wr2EFo"
      },
      "outputs": [],
      "source": [
        "query = \"can you tell me about the Llama LLMs?\"\n",
        "docs = get_docs(query, 5)\n",
        "print(generate(query=query, docs=docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
