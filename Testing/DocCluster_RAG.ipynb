{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prerequisite libraries installation\n",
        "!pip install -qU datasets ollama openai \"semantic-router[local]\" qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALYMqYsamUy2"
      },
      "source": [
        "Data Preparation and Embedding:\n",
        "\n",
        "This step involves loading the dataset, preprocessing it, embedding the text data using a pre-trained model, and storing these embeddings in a vector database (e.g., Qdrant). These embeddings will be used for clustering and retrieval tasks in the subsequent steps.\n",
        "Use LLMs for Hyperparameter Optimization and Clustering:\n",
        "\n",
        "Once the embeddings are prepared, an LLM is used to suggest optimal hyperparameters for clustering. The suggested parameters are then used to perform clustering, and the quality of clustering is evaluated using metrics like silhouette scores. This step iteratively refines the clustering process based on feedback from the LLM.\n",
        "Generative Response Integration:\n",
        "\n",
        "After clustering, retrieval-augmented generation (RAG) is implemented to retrieve relevant documents based on a query. An LLM is then used to generate responses based on these retrieved documents. This step combines the results of the clustering process and uses the LLM to provide meaningful answers to queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up2fEED5l9Ws"
      },
      "source": [
        "Step 1: Data Preparation and Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94628b0afaaa42b186d6f414d25f6a14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "# Data Preparation\n",
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"jamescalam/ai-arxiv2-semantic-chunks\", split=\"train[:10000]\")\n",
        "data = data.map(lambda x: {\"id\": x[\"id\"], \"metadata\": {\"title\": x[\"title\"], \"content\": x[\"content\"]}})\n",
        "data = data.remove_columns([\"title\", \"content\", \"prechunk_id\", \"postchunk_id\", \"arxiv_id\", \"references\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from semantic_router.encoders import HuggingFaceEncoder\n",
        "\n",
        "# Force CPU usage\n",
        "torch.cuda.is_available = lambda : False\n",
        "\n",
        "encoder = HuggingFaceEncoder(name=\"dwzhu/e5-base-4k\")\n",
        "dims = len(encoder([\"this is a test\"])[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Qdrant Setup\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "index_name = \"qdrant-llama-3-rag-1\"\n",
        "client = QdrantClient(\"http://localhost:6333\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "32pFolAvl7hV"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "329ef9c767684481957954042653fba4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "if index_name not in [collection.name for collection in client.get_collections().collections]:\n",
        "    client.create_collection(collection_name=index_name, vectors_config=models.VectorParams(size=dims, distance=models.Distance.COSINE))\n",
        "    time.sleep(1)\n",
        "\n",
        "# Populate Qdrant with embeddings\n",
        "batch_size = 128\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    batch = data[i:i+batch_size]\n",
        "    chunks = [f'{x[\"title\"]}: {x[\"content\"]}' for x in batch[\"metadata\"]]\n",
        "    embeds = encoder(chunks)\n",
        "    points = [models.PointStruct(id=int(id.split('#')[0].replace('.', '')), vector=embed, payload=metadata)\n",
        "              for id, embed, metadata in zip(batch[\"id\"], embeds, batch[\"metadata\"])]\n",
        "    client.upsert(collection_name=index_name, points=points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsfOWPYLl7oU"
      },
      "source": [
        "Step 2: Use LLMs for Hyperparameter Optimization and Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a function to use LLM for suggesting clustering parameters\n",
        "from openai import OpenAI\n",
        "\n",
        "llm_client = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
        "MODEL = 'llama3'\n",
        "\n",
        "def suggest_clustering_params():\n",
        "    query = \"Based on the dataset characteristics, suggest optimal hyperparameters for hierarchical clustering, including the number of clusters and linkage type.\"\n",
        "    messages = [{\"role\": \"user\", \"content\": query}]\n",
        "    response = llm_client.chat.completions.create(model=MODEL, messages=messages)\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To determine the optimal hyperparameters for hierarchical clustering, let's analyze the characteristics of your dataset:\n",
            "\n",
            "1. **Number of features (n_features=12)**: This suggests that your data might be relatively low-dimensional, which could impact the choice of linkage type.\n",
            "2. **Data density**: Since you didn't provide information about the data density, I'll make an assumption based on typical clustering problems. If the data is dense with many samples per feature, a more conservative clustering method like single or average linkage might be suitable. If the data is sparse, a more aggressive approach like complete or ward linkage could work better.\n",
            "3. **Cluster shapes**: Without knowing the exact shape of your clusters (e.g., spherical, linear, or irregular), I'll assume that you're working with general-purpose clustering.\n",
            "\n",
            "Considering these factors, here are some suggested optimal hyperparameters for hierarchical clustering:\n",
            "\n",
            "**Number of clusters (n_clusters)**:\n",
            "To determine the ideal number of clusters, we can use the **Calinski-Harabasz index**, which evaluates the ratio of between-cluster variance to within-cluster variance. A higher value indicates a better separation between clusters.\n",
            "\n",
            "For your dataset with 12 features and an unknown number of clusters, I recommend trying **n_clusters = 4** or **5** as a starting point. This is because hierarchical clustering often requires fewer clusters than traditional K-means clustering methods. You can adjust this value based on the Calinski-Harabasz index or other evaluation metrics.\n",
            "\n",
            "**Linkage type (linkage)**:\n",
            "Given the relatively low dimensionality and potential cluster shapes, I suggest trying two linkage types:\n",
            "\n",
            "1. **Complete (single-linkage): This approach is suitable for dense data with many samples per feature. It's robust to noise and can capture clusters of varying sizes.**\n",
            "2. **Average (UPGMA-linkage): This method is more aggressive than complete linkage and might be better suited for sparse or noisy data. It tends to group smaller clusters together.**\n",
            "\n",
            "You can experiment with both linkage types and evaluate their performance using metrics like the Calinski-Harabasz index, silhouette score, or Davies-Bouldin index.\n",
            "\n",
            "Here's a sample code snippet in Python using the SciPy library:\n",
            "```python\n",
            "import numpy as np\n",
            "from scipy.cluster.hierarchy import fcluster, dendrogram\n",
            "from sklearn.datasets import load_dataset\n",
            "\n",
            "# Load your dataset (replace with your data)\n",
            "data = load_dataset('your_data.csv')\n",
            "\n",
            "# Perform hierarchical clustering with different linkage types\n",
            "n_clusters = 4  # or 5\n",
            "\n",
            "complete_linkage = fcluster(Z, n_clusters, 'complete')\n",
            "average_linkage = fcluster(Z, n_clusters, 'average')\n",
            "\n",
            "# Evaluate performance using Calinski-Harabasz index\n",
            "from scipy.spatial.distance import pdist\n",
            "calinski_harabasz_complete = calinski_harabasz(pdist(data[:, :n_features], metric='euclidean'), complete_linkage)\n",
            "calinski_harabasz_average = calinski_harabasz(pdist(data[:, :n_features], metric='euclidean'), average_linkage)\n",
            "\n",
            "# Visualize the dendrogram to inspect cluster structure\n",
            "dendrogram(Z, colorbar=True, truncate_mode='level')\n",
            "```\n",
            "Remember that these are just suggestions based on general clustering guidelines. The optimal hyperparameters may vary depending on your specific data and problem constraints. Feel free to experiment and adjust these values accordingly!\n"
          ]
        }
      ],
      "source": [
        "# Use the LLM to get suggested parameters\n",
        "suggested_params = suggest_clustering_params()\n",
        "print(suggested_params)  # Parse the response to get the actual parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QFWc3ZUnl7vN"
      },
      "outputs": [],
      "source": [
        "# Perform Clustering with Suggested Parameters\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "def perform_clustering(df_emb, n_clusters=3, linkage='ward'):\n",
        "    model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
        "    labels = model.fit_predict(df_emb)\n",
        "    score = silhouette_score(df_emb, labels)\n",
        "    return labels, score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Silhouette Score: 0.14872944895028972\n",
            "A great question!\n",
            "\n",
            "To suggest optimal hyperparameters for hierarchical clustering, let's analyze the dataset characteristics:\n",
            "\n",
            "**Dataset Characteristics:**\n",
            "\n",
            "1. **Number of samples:** 500\n",
            "2. **Dimensionality:** 20 features (high-dimensional)\n",
            "3. **Distribution:** No clear indication of distributional assumptions (e.g., normality)\n",
            "\n",
            "**Optimal Hyperparameter Suggestions:**\n",
            "\n",
            "Based on these characteristics, here are some optimal hyperparameter suggestions:\n",
            "\n",
            "**Number of Clusters (K):**\n",
            "For a dataset with 500 samples and high dimensionality (20 features), I recommend starting with a smaller number of clusters to avoid over-clustering. Let's try `k=3` to `k=5`.\n",
            "\n",
            "**Linkage Type:**\n",
            "Given the high dimensionality, I suggest using a **ward linkage**, which is more robust to noise and outliers compared to other linkage types like single or complete linkage.\n",
            "\n",
            "**Why these hyperparameters?**\n",
            "\n",
            "1. Starting with a smaller number of clusters (`k=3` to `k=5`) allows for a better separation between clusters and reduces the risk of over-clustering.\n",
            "2. Ward linkage is well-suited for high-dimensional data, as it takes into account both the distance between samples and the similarity between features.\n",
            "\n",
            "**Additional Tips:**\n",
            "\n",
            "1. **Preprocess your data:** Consider applying techniques like PCA (Principal Component Analysis) or t-SNE (t-Distributed Stochastic Neighbor Embedding) to reduce dimensionality and improve clustering results.\n",
            "2. **Monitor Silhouette Coefficients:** As you vary `k`, monitor the silhouette coefficients to ensure that the clusters are well-separated and the data is not being over-clustered.\n",
            "3. **Visualize Clusters:** Use visualization techniques (e.g., dendrograms, scatter plots) to inspect the cluster assignments and identify any patterns or anomalies.\n",
            "\n",
            "By following these suggestions and monitoring the clustering results, you can optimize your hierarchical clustering algorithm for this dataset.\n",
            "Silhouette Score: 0.14872944895028972\n",
            "A great question!\n",
            "\n",
            "To determine the optimal hyperparameters for hierarchical clustering, we'll use a combination of domain knowledge, exploratory data analysis (EDA), and visualization techniques.\n",
            "\n",
            "**Dataset Characteristics:**\n",
            "From your previous message, I understand that:\n",
            "\n",
            "1. The dataset has 2,500 samples.\n",
            "2. There are 20 features, which are likely to be informative but may not all be equally important.\n",
            "3. The data is quite diverse, with a mix of continuous and categorical variables.\n",
            "4. There are no obvious clusters or patterns in the data.\n",
            "\n",
            "**Optimal Hyperparameters:**\n",
            "\n",
            "1. **Number of Clusters (K):** Given the diversity of the dataset, I would recommend a relatively small number of clusters to avoid over-clustering. A good starting point could be 3-5 clusters.\n",
            "2. **Linkage Type:** For a diverse dataset with a mix of feature types, I would suggest using a linkage type that is robust to different distances and dimensionality. The `single` or `complete` linkage methods are often suitable for datasets like this.\n",
            "\n",
            "**Why these choices?**\n",
            "\n",
            "* A small number of clusters (3-5) can help capture meaningful patterns without over-fragmenting the data.\n",
            "* Single or complete linkage methods tend to perform well when there is a mix of feature types and no clear structure in the data. They are also less sensitive to noisy or distant samples, which is important given the diversity of your dataset.\n",
            "\n",
            "**Additional Tips:**\n",
            "\n",
            "1. **Visualize the dendrogram:** Use a dendrogram (or hierarchical clustering plot) to visualize the results and check for meaningful clusters.\n",
            "2. **Evaluate using metrics:** Use metrics like Silhouette Score, Calinski-Harabasz Index, or Davies-Bouldin Index to evaluate the quality of the clustering.\n",
            "3. **Try different linkage methods:** Experiment with other linkage types (e.g., `average`, `weighted`) if the results are not satisfactory with the initial choice.\n",
            "\n",
            "By following these guidelines and exploring the dataset's characteristics, you should be able to find a suitable number of clusters and linkage type for your hierarchical clustering model.\n",
            "Silhouette Score: 0.14872944895028972\n",
            "Based on the characteristics of your dataset, I would recommend the following optimal hyperparameters for hierarchical clustering:\n",
            "\n",
            "**Number of Clusters (K)**: Since you have a moderate-sized dataset with 500 samples, I would suggest setting the number of clusters to 4-6. This is because too few clusters may not capture the inherent structure in your data, while too many clusters can lead to overfitting. A moderate number of clusters will allow for some degree of granularity in identifying meaningful subgroups.\n",
            "\n",
            "**Linkage Type**: Given the high-dimensional nature of your dataset (1000 features), I would recommend using a linkage type that is robust to noise and outliers. Specifically:\n",
            "\n",
            "* **Ward's linkage** (also known as Ward's minimum variance method): This linkage method is particularly well-suited for high-dimensional data, as it is more robust to the presence of noisy or irrelevant features. Ward's linkage also tends to produce clusters with roughly equal sizes.\n",
            "\n",
            "Here are some specific values you can try:\n",
            "\n",
            "* K = 5\n",
            "* Linkage type: Ward's linkage\n",
            "\n",
            "**Additional Tips**\n",
            "\n",
            "1. **Start with a relatively large distance threshold**: Begin with a larger distance threshold (e.g., `0.5`) and gradually decrease it if the resulting dendrogram appears to be too coarse.\n",
            "2. **Use a dendrogram visualization tool**: Use a tool like dendrograms or hierarchical clustering plots to visualize the results of your hierarchical clustering algorithm. This can help you identify the optimal number of clusters (K) based on the structure of the dendrogram.\n",
            "\n",
            "By using these hyperparameters, you should be able to effectively cluster your high-dimensional dataset into meaningful subgroups.\n",
            "\n",
            "Do you have any specific questions about this recommendation or would you like me to elaborate on any of these points?\n",
            "Silhouette Score: 0.14872944895028972\n"
          ]
        }
      ],
      "source": [
        "# Example of parsed parameters (replace with actual parsing logic)\n",
        "n_clusters = 3\n",
        "linkage = 'ward'\n",
        "\n",
        "# Perform clustering using suggested parameters\n",
        "labels, score = perform_clustering(embeds, n_clusters=n_clusters, linkage=linkage)\n",
        "print(f\"Silhouette Score: {score}\")\n",
        "\n",
        "# Iterate based on feedback\n",
        "for _ in range(3):\n",
        "    suggested_params = suggest_clustering_params()\n",
        "    print(suggested_params)  # Update parameters based on response\n",
        "    # Perform clustering again with new parameters\n",
        "    labels, score = perform_clustering(embeds, n_clusters=n_clusters, linkage=linkage)\n",
        "    print(f\"Silhouette Score: {score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbHbrvFgl72D"
      },
      "source": [
        "Step 3: Generative Response Integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aOV-StFYl787"
      },
      "outputs": [],
      "source": [
        "# Retrieval Function\n",
        "def get_docs(query: str, top_k: int) -> list[str]:\n",
        "    xq = encoder([query])\n",
        "    search_result = client.search(collection_name=index_name, limit=top_k, query_vector=xq[0], with_payload=True)\n",
        "    return [point.payload['content'] for point in search_result]\n",
        "\n",
        "# Generate Responses Using LLM\n",
        "def generate(query: str, docs: list[str]):\n",
        "    system_message = (\n",
        "        \"You are a helpful assistant that answers questions about AI using the \"\n",
        "        \"context provided below.\\n\\n\"\n",
        "        \"CONTEXT:\\n\"\n",
        "        \"\\n---\\n\".join(docs)\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "    chat_response = llm_client.chat.completions.create(model=MODEL, messages=messages)\n",
        "    return chat_response.choices[0].message.content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLaMA!\n",
            "\n",
            "LLaMA (Large Language Model Applications) is a family of foundation models developed by Meta AI, a leading artificial intelligence research organization. These models are designed to be highly flexible and versatile, allowing them to be fine-tuned for various natural language processing (NLP) tasks.\n",
            "\n",
            "The LLaMA family includes several models with different sizes and capabilities:\n",
            "\n",
            "1. **LLaMA 2**: This is the largest and most capable of the LLaMA models. It was trained on a massive dataset of text from the internet, books, and other sources, and it can generate text that is similar in style and content to a given input prompt.\n",
            "2. **FLAN (Foundation Language Model)**: FLAN is a smaller, more focused model designed for specific NLP tasks like question answering, sentence completion, or text classification.\n",
            "3. **GLaM (Generative Large Language Model)**: GLaM is another foundation model that's particularly well-suited for generating text based on input prompts.\n",
            "\n",
            "These models have been shown to excel in various NLP benchmarks and are being used to improve a wide range of AI applications, from chatbots and language translation systems to content generation and more.\n",
            "\n",
            "Some key characteristics of the LLaMA models include:\n",
            "\n",
            "* **Large-scale training**: The LLaMA models were trained on massive datasets of text, which allows them to learn complex patterns and relationships in language.\n",
            "* **Foundation model architecture**: The LLaMA models are based on transformer architectures, which are particularly well-suited for natural language processing tasks.\n",
            "* **Fine-tuning**: These models can be fine-tuned for specific NLP tasks by adjusting their parameters using a smaller dataset related to that task.\n",
            "\n",
            "Overall, the LLaMA models represent significant advancements in AI research and have the potential to enable new applications and innovations in various fields.\n"
          ]
        }
      ],
      "source": [
        "# Example Usage\n",
        "query = \"can you tell me about the Llama LLMs?\"\n",
        "docs = get_docs(query, 5)\n",
        "print(generate(query=query, docs=docs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbqfcesfl8D7"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
